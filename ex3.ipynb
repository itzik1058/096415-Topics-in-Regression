{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "c8fafa882b704a0e60fd895cf4761eb18eaa713da27b01e55662e8e5ffd7eae5"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    eval = {}\n",
    "    n, d = X.shape\n",
    "    eps = model.predict(X) - y.flatten()\n",
    "    mse = np.sum(np.square(eps)) / n\n",
    "    var = np.var(eps)\n",
    "    Q_inv = np.linalg.inv(X.T @ X / n)\n",
    "    omega = X.T @ np.diag(eps) @ X / n\n",
    "    eval['pred_hat'] = mse + 2 * np.trace(Q_inv @ omega @ Q_inv) / n\n",
    "    eval['BIC'] = mse + 2 * np.log(n) * d * var / n\n",
    "    eval['c_p_mallow'] = mse + 2 * d * var / n\n",
    "    eval['cross_validation'] = cross_validation(X, y)\n",
    "    return eval\n",
    "\n",
    "\n",
    "def cross_validation(X, y, k=5):\n",
    "    folds = KFold(k)\n",
    "    mse = 0\n",
    "    for train_idx, test_idx in folds.split(X):\n",
    "        model = sm.OLS(y[train_idx], X[train_idx]).fit()\n",
    "        mse += np.sum(np.square(model.predict(X[test_idx]) - y[test_idx].flatten()))\n",
    "    return mse / X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model 1 {'pred_hat': 1.036368622218773, 'BIC': 1.1293829363951629, 'c_p_mallow': 1.0530033672902521, 'cross_validation': 1.0605033737602012}\nModel 2 {'pred_hat': 1.4583462710170072, 'BIC': 1.580824200221883, 'c_p_mallow': 1.4739138978323705, 'cross_validation': 1.4762143190493433}\nModel 3 {'pred_hat': 1.9065414998196406, 'BIC': 2.0783542760985156, 'c_p_mallow': 1.9377962785052085, 'cross_validation': 2.00072390204187}\nModel 4 {'pred_hat': 1.014424668771491, 'BIC': 1.1517100030268406, 'c_p_mallow': 1.0395132229610944, 'cross_validation': 1.0423663451131129}\nModel 5 {'pred_hat': 1.0356012403150814, 'BIC': 1.170169847788936, 'c_p_mallow': 1.056174754660546, 'cross_validation': 1.0816330343548475}\nModel 6 {'pred_hat': 1.4549054839462867, 'BIC': 1.6350293677534635, 'c_p_mallow': 1.4757487937437261, 'cross_validation': 1.5699247673931107}\nModel 7 {'pred_hat': 1.0177277486562684, 'BIC': 1.1956041353594216, 'c_p_mallow': 1.04623591578404, 'cross_validation': 1.0790084322353986}\nThe best model according to pred_hat criterion is Model 4\nThe best model according to BIC criterion is Model 1\nThe best model according to c_p_mallow criterion is Model 4\nThe best model according to cross_validation criterion is Model 4\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "tips = sn.load_dataset('tips')\n",
    "tips['smoker'] = (tips['smoker'] == 'Yes').astype(int)\n",
    "y = tips['tip'].to_numpy()\n",
    "features = ['total_bill', 'size', 'smoker']\n",
    "for i in range(1, 4):\n",
    "    models.extend([list(x) for x in combinations(features, i)])\n",
    "evaluations = {}\n",
    "for idx, model in enumerate(models):\n",
    "    X = tips[model]\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "    ols = sm.OLS(y, X)\n",
    "    model = ols.fit()\n",
    "    evaluations[f'Model {idx + 1}'] = evaluate_model(model, X, y)\n",
    "for n, m in evaluations.items():\n",
    "    print(n, m)\n",
    "criterion_evaluation = {}\n",
    "for model, metrics in evaluations.items():\n",
    "    for criterion, value in metrics.items():\n",
    "        if criterion not in criterion_evaluation:\n",
    "            criterion_evaluation[criterion] = {}\n",
    "        criterion_evaluation[criterion][model] = value\n",
    "for criterion, scores in criterion_evaluation.items():\n",
    "    print(f'The best model according to {criterion} criterion is {min(scores, key=scores.get)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}